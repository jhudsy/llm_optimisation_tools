# Optimise-MCP Configuration File
# Configuration for the 5-agent optimization workflow

# =============================================================================
# Optimization Workflow Configuration
# =============================================================================
modeller_checker:
  # Agent LLM Configuration
  # Each agent can use a different LLM configuration
  formulator:
    provider: "ollama"
    model: "qwen3"
    base_url: "http://127.0.0.1:11434"
    temperature: 0.5
  
  equation_checker:
    provider: "ollama"
    model: "qwen3"
    base_url: "http://127.0.0.1:11434"
    temperature: 0.3
  
  translator:
    provider: "ollama"
    model: "qwen3"
    base_url: "http://127.0.0.1:11434"
    temperature: 0.4
  
  code_checker:
    provider: "ollama"
    model: "qwen3"
    base_url: "http://127.0.0.1:11434"
    temperature: 0.2
  
  solver_executor:
    provider: "ollama"
    model: "qwen3"
    base_url: "http://127.0.0.1:11434"
    temperature: 0.3
  
  # Workflow Settings
  workflow:
    max_iterations: 10
    verbose: false
  
  # MCP Server Settings
  mcp_server:
    stdio_enabled: true
    http_enabled: true
    http_host: "127.0.0.1"
    http_port: 8767
    log_level: "INFO"
  
  # LLM Defaults
  llm_defaults:
    default_max_tokens: 2048

# =============================================================================
# MiniZinc Solver Configuration (used by workflow)
# =============================================================================
mzn:
  solver:
    backend: "coinbc"  # Options: coinbc, gecode, chuffed, or-tools, etc.
    use_python_bindings: true  # If false, falls back to CLI
    time_limit: 60  # seconds
  
  validation:
    strict_mode: false  # Stricter validation rules
    default_temperature: 0.5

# =============================================================================
# Global Settings
# =============================================================================
global:
  log_level: "INFO"
  log_file: null  # Set to a path to enable file logging
  debug_mode: false

# =============================================================================
# Example Cloud Provider Configurations
# =============================================================================
# You can mix different providers for different agents

# OpenAI Configuration Example:
# modeller_checker:
#   formulator:
#     provider: "openai"
#     model: "gpt-4"
#     api_key: "${OPENAI_API_KEY}"
#     temperature: 0.5
#     max_tokens: 2000

# Anthropic Configuration Example:
# modeller_checker:
#   equation_checker:
#     provider: "anthropic"
#     model: "claude-3-sonnet-20240229"
#     api_key: "${ANTHROPIC_API_KEY}"
#     temperature: 0.3
#     max_tokens: 1500

# Azure OpenAI Configuration Example:
# modeller_checker:
#   translator:
#     provider: "azure"
#     model: "gpt-4"
#     api_key: "${AZURE_OPENAI_API_KEY}"
#     azure_endpoint: "https://your-resource.openai.azure.com/"
#     api_version: "2024-02-15-preview"
#     temperature: 0.4

# =============================================================================
# MiniZinc Solver Backend Options
# =============================================================================
# Available MiniZinc solvers (if installed):
# - coinbc: Coin-OR Branch and Cut (default, good for MILP)
# - gecode: Gecode constraint solver (good for CP)
# - chuffed: Lazy clause generation solver
# - or-tools: Google OR-Tools
# To see available solvers: minizinc --solvers
